{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n17S-aOBHWlO"
   },
   "source": [
    "# EE 599 HW 4: GPU CUDA Programming\n",
    "\n",
    "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any).\n",
    "\n",
    "Prerequisites: set the runtime type to GPU. (Runtime -> Change Runtime Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrRh8DhCHWlP"
   },
   "source": [
    "The `nvidia-smi` cli tells you about the GPU information on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtTmgmYEHWlP",
    "outputId": "e52d7e23-3ff8-49bb-fb30-d17591b00dad"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tue Mar 19 03:48:26 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmS-6QZdHWlP"
   },
   "source": [
    "The GPU compiler for c++ from Nvidia is called `nvcc`, and is already installed on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hDJCtnGHWlP",
    "outputId": "b3cc14e8-088d-4bc9-b613-677128d8900e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHVR6mJPHWlP"
   },
   "source": [
    "## Vector Add\n",
    "\n",
    "Similar to the previous assignment, we use `%%writefile` command to save the content of a notebook cell directly into a file, which can then be compiled and executed using command-line instructions.\n",
    "\n",
    "The example file `vector_add.cu` implements the vector addition using 64K threads with CUDA programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5sAb_VfHWlP",
    "outputId": "458d7085-6cf9-4e1b-f9a9-c383f06889d1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing vector_add.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile vector_add.cu\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "\n",
    "#define size 65536\n",
    "\n",
    "__global__ void vector_add(int *A, int *B, int *C) {\n",
    "    int my_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    C[my_id] = A[my_id] + B[my_id];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int i;\n",
    "    int *A = (int *)malloc(sizeof(int) * size);\n",
    "    int *B = (int *)malloc(sizeof(int) * size);\n",
    "    int *C = (int *)malloc(sizeof(int) * size);\n",
    "\n",
    "    for (i = 0; i < size; i++) {\n",
    "        A[i] = 1;\n",
    "        B[i] = 2;\n",
    "    }\n",
    "\n",
    "    int *gpu_A, *gpu_B, *gpu_C;\n",
    "    cudaMalloc((void **)&gpu_A, sizeof(int) * size);\n",
    "    cudaMalloc((void **)&gpu_B, sizeof(int) * size);\n",
    "    cudaMalloc((void **)&gpu_C, sizeof(int) * size);\n",
    "\n",
    "    struct timespec start, stop;\n",
    "    double time;\n",
    "\n",
    "    cudaMemcpy(gpu_A, A, sizeof(int) * size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(gpu_B, B, sizeof(int) * size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 dimGrid(64);\n",
    "    dim3 dimBlock(1024);\n",
    "\n",
    "    if (clock_gettime(CLOCK_REALTIME, &start) == -1) {\n",
    "        perror(\"clock gettime\");\n",
    "    }\n",
    "    vector_add<<<dimGrid, dimBlock>>>(gpu_A, gpu_B, gpu_C);\n",
    "    cudaMemcpy(C, gpu_C, sizeof(int) * size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    if (clock_gettime(CLOCK_REALTIME, &stop) == -1) {\n",
    "        perror(\"clock gettime\");\n",
    "    }\n",
    "    time = (stop.tv_sec - start.tv_sec) + (double)(stop.tv_nsec - start.tv_nsec) / 1e9;\n",
    "\n",
    "    printf(\"Execution time = %f sec\\n\", time);\n",
    "\n",
    "    for (i = 0; i < 10; i++) {\n",
    "        printf(\"C[%d]=%d \", i, C[i]);\n",
    "    }\n",
    "\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "    cudaFree(gpu_A);\n",
    "    cudaFree(gpu_B);\n",
    "    cudaFree(gpu_C);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akjXQAX0HWlQ"
   },
   "source": [
    "Compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIR1cxuGHWlQ",
    "outputId": "0f04a1a8-703d-4c57-c930-c45473d8500d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execution time = 0.101225 sec\n",
      "C[0]=3 C[1]=3 C[2]=3 C[3]=3 C[4]=3 C[5]=3 C[6]=3 C[7]=3 C[8]=3 C[9]=3 "
     ]
    }
   ],
   "source": [
    "!nvcc vector_add.cu -o vector_add.out && ./vector_add.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3BrZgzHHWlQ"
   },
   "source": [
    "## Matrix Multiplacation\n",
    "\n",
    "### **TODO 1:**\n",
    "\n",
    "Implement unoptimized matrix multiplication using global memory only:\n",
    "\n",
    "- Thread block configuration: 16 × 16\n",
    "- Grid configuration: 64 × 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bU347qasHWlQ",
    "outputId": "bc440dda-9776-40ef-c544-412767ad80f1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting mat_mul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile mat_mul.cu\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "\n",
    "#define n 1024\n",
    "\n",
    "// TODO: Write GPU kernel to perform matrix multiplication\n",
    "__global__ void matrix_multipl(int *A, int *B, int *C, int thread_num) {\n",
    "\n",
    "    int bx = blockIdx.x, by = blockIdx.y;\n",
    "    int tx = threadIdx.x, ty = threadIdx.y;\n",
    "\n",
    "    int Row = by * thread_num + ty;\n",
    "    int Col = bx * thread_num + tx;\n",
    "    int Cvalue = 0;\n",
    "\n",
    "    for (int k = 0; k < n; ++k) {\n",
    "        Cvalue += A[Row * n + k] * B[k * n + Col];\n",
    "    }\n",
    "    C[Row * n + Col] = Cvalue;\n",
    "}\n",
    "\n",
    "\n",
    "int main() {\n",
    "    int i, j;\n",
    "    int *A = (int *) malloc(sizeof(int)*n*n);\n",
    "    int *B = (int *) malloc(sizeof(int)*n*n);\n",
    "    int *C = (int *) malloc(sizeof(int)*n*n);\n",
    "\n",
    "    for (i = 0; i < n; i++) {\n",
    "        for (j = 0; j < n; j++) {\n",
    "            A[i*n + j] = i;\n",
    "            B[i*n + j] = i + j;\n",
    "            C[i*n + j] = 0;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // TODO: Allocate device memory, use variable names: gpu_A, gpu_B and gpu_C\n",
    "    int *gpu_A, *gpu_B, *gpu_C;\n",
    "    cudaMalloc((void **)&gpu_A, sizeof(int)*n*n);\n",
    "    cudaMalloc((void **)&gpu_B, sizeof(int)*n*n);\n",
    "    cudaMalloc((void **)&gpu_C, sizeof(int)*n*n);\n",
    "\n",
    "\n",
    "    // TODO: Transfer data to device\n",
    "    cudaMemcpy(gpu_A, A, sizeof(int)*n*n, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(gpu_B, B, sizeof(int)*n*n, cudaMemcpyHostToDevice);\n",
    "\n",
    "\n",
    "    // TODO: Set grid and block sizes\n",
    "    dim3 dimGrid(64, 64);\n",
    "    dim3 dimBlock(16, 16);\n",
    "\n",
    "\n",
    "    struct timespec start, stop;\n",
    "    double time;\n",
    "\n",
    "    if( clock_gettime( CLOCK_REALTIME, &start) == -1 ) { perror( \"clock gettime\" );}\n",
    "\n",
    "    // TODO: Launch kernel\n",
    "    matrix_multipl<<<dimGrid, dimBlock>>>(gpu_A, gpu_B, gpu_C, 16);\n",
    "\n",
    "\n",
    "    // TODO: Transfer data back to host\n",
    "    cudaMemcpy(C, gpu_C, sizeof(int)*n*n, cudaMemcpyDeviceToHost);\n",
    "\n",
    "\n",
    "    if( clock_gettime( CLOCK_REALTIME, &stop) == -1 ) { perror( \"clock gettime\" );}\n",
    "    time = (stop.tv_sec - start.tv_sec)+ (double)(stop.tv_nsec - start.tv_nsec)/1e9;\n",
    "\n",
    "    // Print results\n",
    "    printf(\"Number of FLOPs = %llu, Execution time = %f sec,\\n%lf MFLOPs per sec\\n\",\n",
    "        2ULL * n * n * n, time, 1 / time / 1e6 * 2 * n * n * n);\n",
    "\n",
    "    printf(\"C[100][100]=%d\\n\", C[100*n + 100]);\n",
    "\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "    cudaFree(gpu_A);\n",
    "    cudaFree(gpu_B);\n",
    "    cudaFree(gpu_C);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6Cb0sY7HWlQ",
    "outputId": "8ca0b888-5ccb-4201-bdb4-8ca05fef451d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of FLOPs = 2147483648, Execution time = 0.050882 sec,\n",
      "42204.955546 MFLOPs per sec\n",
      "C[100][100]=62617600\n"
     ]
    }
   ],
   "source": [
    "!nvcc mat_mul.cu -o mat_mul.out && ./mat_mul.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVe2Yl80HWlQ"
   },
   "source": [
    "### **TODO 2**:\n",
    "\n",
    "Implement block matrix multiplication using shared memory.\n",
    "- Thread block configuration: 32 × 32\n",
    "- Grid configuration: 32 × 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "mIaqKVjBHWlQ",
    "outputId": "ff529a48-358a-48a4-e2b9-ecc5f584a144",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting blocked_mat_mul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile blocked_mat_mul.cu\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "\n",
    "#define n 1024\n",
    "#define block_size 32\n",
    "\n",
    "// TODO: Write GPU kernel to perform matrix multiplication\n",
    "__global__ void matrix_multipl_share(int *A, int *B, int *C) {\n",
    "    __shared__ int ds_A[block_size][block_size];\n",
    "    __shared__ int ds_B[block_size][block_size];\n",
    "\n",
    "    int bx = blockIdx.x, by = blockIdx.y;\n",
    "    int tx = threadIdx.x, ty = threadIdx.y;\n",
    "\n",
    "    int Row = by * block_size + ty;\n",
    "    int Col = bx * block_size + tx;\n",
    "    int Cvalue = 0;\n",
    "\n",
    "    for (int t = 0; t < n / block_size; ++t) {\n",
    "        ds_A[ty][tx] = A[Row * n + t * block_size + tx];\n",
    "        ds_B[ty][tx] = B[(t * block_size + ty) * n + Col];\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        for (int k = 0; k < block_size; ++k) {\n",
    "            Cvalue += ds_A[ty][k] * ds_B[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    C[Row * n + Col] = Cvalue;\n",
    "}\n",
    "\n",
    "\n",
    "int main() {\n",
    "    int i, j;\n",
    "    int *A = (int *) malloc(sizeof(int)*n*n);\n",
    "    int *B = (int *) malloc(sizeof(int)*n*n);\n",
    "    int *C = (int *) malloc(sizeof(int)*n*n);\n",
    "\n",
    "    for (i = 0; i < n; i++) {\n",
    "        for (j = 0; j < n; j++) {\n",
    "            A[i*n + j] = i;\n",
    "            B[i*n + j] = i + j;\n",
    "            C[i*n + j] = 0;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // TODO: Allocate device memory, use variable names: gpu_A, gpu_B and gpu_C\n",
    "    int *gpu_A, *gpu_B, *gpu_C;\n",
    "    cudaMalloc((void **)&gpu_A, sizeof(int)*n*n);\n",
    "    cudaMalloc((void **)&gpu_B, sizeof(int)*n*n);\n",
    "    cudaMalloc((void **)&gpu_C, sizeof(int)*n*n);\n",
    "\n",
    "\n",
    "    // TODO: Transfer data to device\n",
    "    cudaMemcpy(gpu_A, A, sizeof(int)*n*n, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(gpu_B, B, sizeof(int)*n*n, cudaMemcpyHostToDevice);\n",
    "\n",
    "\n",
    "    // TODO: Set grid and block sizes\n",
    "    dim3 dimGrid(32, 32);\n",
    "    dim3 dimBlock(32, 32);\n",
    "\n",
    "\n",
    "    struct timespec start, stop;\n",
    "    double time;\n",
    "\n",
    "    if( clock_gettime( CLOCK_REALTIME, &start) == -1 ) { perror( \"clock gettime\" );}\n",
    "\n",
    "    // TODO: Launch kernel\n",
    "    matrix_multipl_share<<<dimGrid, dimBlock>>>(gpu_A, gpu_B, gpu_C);\n",
    "\n",
    "\n",
    "    // TODO: Transfer data back to host\n",
    "    cudaMemcpy(C, gpu_C, sizeof(int)*n*n, cudaMemcpyDeviceToHost);\n",
    "\n",
    "\n",
    "    if( clock_gettime( CLOCK_REALTIME, &stop) == -1 ) { perror( \"clock gettime\" );}\n",
    "    time = (stop.tv_sec - start.tv_sec)+ (double)(stop.tv_nsec - start.tv_nsec)/1e9;\n",
    "\n",
    "    // Print results\n",
    "    printf(\"Number of FLOPs = %llu, Execution time = %f sec,\\n%lf MFLOPs per sec\\n\",\n",
    "        2ULL * n * n * n, time, 1 / time / 1e6 * 2 * n * n * n);\n",
    "\n",
    "    printf(\"C[100][100]=%d\\n\", C[100*n + 100]);\n",
    "\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "    cudaFree(gpu_A);\n",
    "    cudaFree(gpu_B);\n",
    "    cudaFree(gpu_C);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Hv3XE6xtHWlQ",
    "outputId": "c7f6aca3-ac56-473b-ac6e-1dca2f40bbdb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of FLOPs = 2147483648, Execution time = 0.006688 sec,\n",
      "321115.784991 MFLOPs per sec\n",
      "C[100][100]=62617600\n"
     ]
    }
   ],
   "source": [
    "!nvcc blocked_mat_mul.cu -o blocked_mat_mul.out && ./blocked_mat_mul.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdDJVIiCHWlQ"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5xCvTr517YI"
   },
   "source": [
    "# EE 599 HW 6: Distributed ML\n",
    "\n",
    "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any).\n",
    "\n",
    "Prerequisites: set the runtime type to GPU. (Runtime -> Change Runtime Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xpa4VriY17YJ"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "Prepare utilization functions and save as local files.\n",
    "\n",
    "* `set_random_seed`: Set random seed for reproducibility.\n",
    "* `load_data`: Load CIFAR-10 dataset and apply transformations.\n",
    "* `create_dataloader`: Create data loaders for training and testing.\n",
    "* `Net`: Define a simple convolution neural network architecture.\n",
    "* `data_split`: Split the dataset into multiple subsets for each client.\n",
    "* `plot_data_split`: Plot the data distribution for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MfNZn1l317YK",
    "outputId": "b379f81e-d72e-4eaa-bceb-7ee2352116fb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    train_data = torchvision.datasets.CIFAR10(\n",
    "        root=\"./\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.CIFAR10(\n",
    "        root=\"./\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def create_dataloader(train_data, test_data, batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_data, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_data, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def data_split(dataset, num_clients, split_method, alpha=0.1):\n",
    "    if split_method == \"iid\":\n",
    "        data_split = torch.utils.data.random_split(\n",
    "            dataset, [len(dataset) // num_clients] * num_clients\n",
    "        )\n",
    "\n",
    "    elif split_method == \"non-iid\":\n",
    "        min_size = 0\n",
    "        num_classes = len(dataset.classes)\n",
    "\n",
    "        while min_size < 10:\n",
    "            idx_batch = [[] for _ in range(num_clients)]\n",
    "            # for each class in the dataset\n",
    "            for k in range(num_classes):\n",
    "                idx_k = np.where(np.array(dataset.targets) == k)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "                ## Balance\n",
    "                proportions = np.array(\n",
    "                    [\n",
    "                        p * (len(idx_j) < len(dataset) / num_clients)\n",
    "                        for p, idx_j in zip(proportions, idx_batch)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                proportions = proportions / proportions.sum()\n",
    "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "                idx_batch = [\n",
    "                    idx_j + idx.tolist()\n",
    "                    for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))\n",
    "                ]\n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "        data_split = [\n",
    "            torch.utils.data.Subset(dataset, idx_batch[i]) for i in range(num_clients)\n",
    "        ]\n",
    "\n",
    "    return data_split\n",
    "\n",
    "\n",
    "def plot_data_split(data_split, num_clients, num_classes, file_name):\n",
    "    data_per_client = []\n",
    "    for i in range(num_clients):\n",
    "        data_per_class = [0] * num_classes\n",
    "        for idx in data_split[i].indices:\n",
    "            label = data_split[i].dataset.targets[idx]\n",
    "            data_per_class[label] += 1\n",
    "        data_per_client.append(data_per_class)\n",
    "\n",
    "    x = [f\"client {i}\" for i in range(1, num_clients + 1)]\n",
    "\n",
    "    y = np.array(data_per_client)\n",
    "    y_t = y.transpose()\n",
    "\n",
    "    cmap = matplotlib.colormaps[\"tab10\"]\n",
    "    colors = cmap(np.arange(10))\n",
    "\n",
    "    for i in range(10):\n",
    "        plt.bar(\n",
    "            x,\n",
    "            y_t[i],\n",
    "            bottom=np.sum(y_t[:i], axis=0),\n",
    "            color=colors[i],\n",
    "            label=f\"Class {i}\",\n",
    "        )\n",
    "\n",
    "    # put legent outside\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.title(\"Data distribution\")\n",
    "    plt.savefig(f\"{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swMaYSEc17YL"
   },
   "source": [
    "## Centralized SGD Training\n",
    "\n",
    "Recall that the function `loss.backward()` computes gradients for each of the parameters in the model based on the forward path, `optimizer.step()` applies the gradients to update the parameters, and `optimizer.zero_grad()` clears the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gf7BYCOe17YL",
    "outputId": "cf267494-940b-4d98-e236-4049a5a91ce4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 65404312.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n",
      "Files already downloaded and verified\n",
      "Epoch: 0 Batch: 200/500 | Loss: 2.2050\n",
      "Epoch: 0 Batch: 400/500 | Loss: 1.7740\n",
      "Epoch: 1 Batch: 200/500 | Loss: 1.5161\n",
      "Epoch: 1 Batch: 400/500 | Loss: 1.4392\n",
      "Epoch: 2 Batch: 200/500 | Loss: 1.3234\n",
      "Epoch: 2 Batch: 400/500 | Loss: 1.2929\n",
      "Test Accuracy: 0.5565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "from utils import load_data, create_dataloader, set_random_seed, Net\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # fix randomness\n",
    "    set_random_seed(42)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device {device}\")\n",
    "\n",
    "    # define model and move to device\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # define optimizer and loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    num_epochs = 3\n",
    "    batch_size = 100\n",
    "\n",
    "    # load cifar10 dataset\n",
    "    train_data, test_data = load_data()\n",
    "    train_loader, test_loader = create_dataloader(\n",
    "        train_data, test_data, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch_idx} Batch: {batch_idx+1}/{len(train_loader)} | Loss: {running_loss/200:.4f}\"\n",
    "                )\n",
    "                running_loss = 0\n",
    "\n",
    "    # evaluate model on testset after training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\") # modified \"logging info\" to \"print\" so that the message can appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on7cpEbk17YL"
   },
   "source": [
    "## Install mpi4py package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oICQ34dl17YL"
   },
   "source": [
    "MPI, or Message Passing Interface, is a standardized and portable message-passing system designed to enable processes to communicate in a parallel computing environment. MPI has become the de facto standard for high-performance parallel computing in a wide range of applications, from simulations in scientific research to large-scale data processing. At its core, MPI provides various communication mechanisms, including point-to-point and collective operations, allowing data to be exchanged between processes irrespective of their physical location—be it on the same machine or across a vast cluster of computers. By abstracting the complexities of inter-process communication, MPI empowers developers to craft scalable parallel software efficiently and effectively.\n",
    "\n",
    "`mpirun` is a command-line utility in most MPI implementations, used to start parallel jobs in distributed computing environments. It launches a specified number of processes on different nodes, enabling them to work together on MPI-enabled applications. The number of processes and their distribution can be controlled by various command-line options and arguments provided to `mpirun`. For instance, using `-n 4` would initiate four parallel processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Gi7JSG1N17YW",
    "outputId": "cff84863-f88e-4709-f0db-deefeb7df801",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting mpi4py\n",
      "  Downloading mpi4py-3.1.6.tar.gz (2.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Building wheels for collected packages: mpi4py\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for mpi4py: filename=mpi4py-3.1.6-cp310-cp310-linux_x86_64.whl size=2746318 sha256=4eedb41c76f1071cd06299e95ca9002f70d4b168441dd8a9a879d89a4ee09932\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/ca/89/8fc1fb1c620afca13bb41c630b1f948bbf446e0aaa4b762e10\n",
      "Successfully built mpi4py\n",
      "Installing collected packages: mpi4py\n",
      "Successfully installed mpi4py-3.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWEOJFY_17YW"
   },
   "source": [
    "In Colab, we need to set the following environment variables to allow for the execution of MPI as the root user and enable oversubscription to control resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c4o_BGNa17YW",
    "outputId": "63a660fe-9b14-4075-e9c0-40e0edb02393",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: OMPI_ALLOW_RUN_AS_ROOT=1\n",
      "env: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1\n",
      "env: OMPI_MCA_rmaps_base_oversubscribe=true\n"
     ]
    }
   ],
   "source": [
    "%env OMPI_ALLOW_RUN_AS_ROOT=1\n",
    "%env OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1\n",
    "%env OMPI_MCA_rmaps_base_oversubscribe=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl7SoW1z17YW"
   },
   "source": [
    "## Distributed Training\n",
    "\n",
    "### **TODO 1:** Data parallel distributed SGD training without parameter server.\n",
    "In this task, you'll simulate data parallel distributed training without a parameter server using MPI. Each rank will simulate a single GPU within a single machine. During the training process, each rank will handle fetching corresponding data and training the model on its allocated portion. Gradients will be synchronized across ranks using the `all_reduce` function.\n",
    "\n",
    "To access gradients of the parameters in the model, you can iterate over the model parameters:\n",
    "```\n",
    "for param in model.parameters():\n",
    "    print(param.grad)\n",
    "```\n",
    "Note `param.grad` is initially set as None and becomes a Tensor the first time a call to `loss.backward()`.\n",
    "\n",
    "The training loss at each printing step should be close to the coresponding one from the centralized implmentaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GI31GaZ217YW",
    "outputId": "d369d4c8-f000-4c6d-e010-e63c2d61eb4d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing serverless.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile serverless.py\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from utils import load_data, create_dataloader, set_random_seed, Net\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # fix randomness\n",
    "    set_random_seed(42)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Rank {rank} is using device {device}\")\n",
    "\n",
    "    # define model and move to device\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # define optimizer and loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    num_epochs = 3\n",
    "    batch_size = 100\n",
    "\n",
    "    # load cifar10 dataset\n",
    "    train_data, test_data = load_data()\n",
    "    train_loader, test_loader = create_dataloader(\n",
    "        train_data, test_data, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # compute number of samples for each rank\n",
    "    num_samples_per_rank = int(batch_size / size)\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch_idx, (data_global, target_global) in enumerate(train_loader):\n",
    "            data_split = torch.split(data_global, num_samples_per_rank, dim=0)\n",
    "            target_split = torch.split(target_global, num_samples_per_rank, dim=0)\n",
    "\n",
    "            data_local = data_split[rank].to(device)\n",
    "            target_local = target_split[rank].to(device)\n",
    "\n",
    "            # compute loss\n",
    "            output = model(data_local)\n",
    "            loss = criterion(output, target_local)\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # TODO: reduce loss to rank 0, this is not necessary but just for debugging\n",
    "            # remember to multiply loss by number of local samples before reduce\n",
    "\n",
    "            loss *= len(data_local)\n",
    "            loss = comm.reduce(loss, op=MPI.SUM, root=0)\n",
    "\n",
    "            # TODO: allreduce gradients for each layer\n",
    "            # remember to multiply gradients by number of local samples before allreduce\n",
    "            # and divide gradients by number of global samples after allreduce\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.grad *= len(data_local)\n",
    "                param.grad = comm.allreduce(param.grad, op=MPI.SUM)\n",
    "                param.grad /= len(data_global)\n",
    "\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print statistics\n",
    "            if rank == 0:\n",
    "                loss /= data_global.shape[0]\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if (batch_idx + 1) % 200 == 0:\n",
    "                    logging.info(\n",
    "                        f\"Epoch: {epoch_idx} Batch: {batch_idx+1}/{len(train_loader)} | Loss: {running_loss/200:.4f}\"\n",
    "                    )\n",
    "                    running_loss = 0\n",
    "\n",
    "    # evaluate model on testset after training on rank 0\n",
    "    model.eval()\n",
    "    if rank == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target in test_loader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            accuracy = correct / total\n",
    "            logging.info(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pR-7Jyks17YX"
   },
   "source": [
    "Launch MPI with 4 processes to run serverless.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "64OLLNN817YX",
    "outputId": "005c2335-a04c-40ea-c747-9f620082f2e7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:root:Rank 3 is using device cuda\n",
      "INFO:root:Rank 1 is using device cuda\n",
      "INFO:root:Rank 0 is using device cuda\n",
      "INFO:root:Rank 2 is using device cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO:root:Epoch: 0 Batch: 200/500 | Loss: 2.2050\n",
      "INFO:root:Epoch: 0 Batch: 400/500 | Loss: 1.7737\n",
      "INFO:root:Epoch: 1 Batch: 200/500 | Loss: 1.5127\n",
      "INFO:root:Epoch: 1 Batch: 400/500 | Loss: 1.4385\n",
      "INFO:root:Epoch: 2 Batch: 200/500 | Loss: 1.3218\n",
      "INFO:root:Epoch: 2 Batch: 400/500 | Loss: 1.2896\n",
      "INFO:root:Test Accuracy: 0.5577\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python serverless.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxsKODfY17YX"
   },
   "source": [
    "### **TODO 2:** Data parallel distributed SGD training with parameter server.\n",
    "\n",
    "In this task, you will implement data parallel distributed training with a parameter server. Assume multiple machines, each equipped with one GPU (simulated by ranks other than 0), cooperate in the training process. Each rank (excluding rank 0) will be responsible for sending gradients to the server (rank 0), which will receive, average these gradients, and send them back for further updates.\n",
    "\n",
    "The training loss at each printing step should be close to the coresponding one from the centralized implmentaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34om44jA17YX",
    "outputId": "aae890aa-53af-4e31-85f0-17b1e05c5b89",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing parameter_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parameter_server.py\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from utils import load_data, create_dataloader, set_random_seed, Net\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # fix randomness\n",
    "    set_random_seed(42)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Rank {rank} is using device {device}\")\n",
    "\n",
    "    # define model and move to device\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # define optimizer and loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    num_epochs = 3\n",
    "    batch_size = 100\n",
    "\n",
    "    # load cifar10 dataset\n",
    "    train_data, test_data = load_data()\n",
    "    train_loader, test_loader = create_dataloader(\n",
    "        train_data, test_data, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # compute number of samples for each rank\n",
    "    # note that rank 0 is the parameter server, and we have size-1 workers\n",
    "    num_samples_per_rank = int(batch_size / (size - 1))\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch_idx, (data_global, target_global) in enumerate(train_loader):\n",
    "            if rank == 0:\n",
    "                data_split = torch.split(data_global, num_samples_per_rank, dim=0)\n",
    "                target_split = torch.split(target_global, num_samples_per_rank, dim=0)\n",
    "\n",
    "                # TODO: send local data and target to other ranks\n",
    "                for i in range(1, size):\n",
    "                    data_local = data_split[i-1]\n",
    "                    target_local = target_split[i-1]\n",
    "                    comm.send(data_local, dest=i, tag=0)\n",
    "                    comm.send(target_local, dest=i, tag=1)\n",
    "\n",
    "\n",
    "                # TODO: receive loss from other ranks and sum them up\n",
    "                loss = 0\n",
    "                for i in range(1, size):\n",
    "                    loss += comm.recv(source=i, tag=2)\n",
    "\n",
    "\n",
    "                # TODO: compute averaged loss by dividing number of global samples\n",
    "                # accumulate averged loss to running loss\n",
    "                running_loss += loss.item()/len(data_global)\n",
    "\n",
    "\n",
    "                # TODO: receive gradients from other ranks and sum them up\n",
    "                # remember to divide gradients by number of global samples\n",
    "                for param in model.parameters():\n",
    "                    param.grad = torch.zeros_like(param.data)\n",
    "\n",
    "                    for i in range(1, size):\n",
    "                        param.grad += comm.recv(source=i, tag=3)\n",
    "\n",
    "                    param.grad /= len(data_global)\n",
    "\n",
    "\n",
    "                # TODO: update parameters and send updated model to other ranks\n",
    "                optimizer.step()\n",
    "\n",
    "                for i in range(1, size):\n",
    "                    comm.send(model, dest=i, tag=10)\n",
    "\n",
    "                # clear gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # print statistics\n",
    "                if (batch_idx + 1) % 200 == 0:\n",
    "                    logging.info(\n",
    "                        f\"Epoch: {epoch_idx} Batch: {batch_idx+1}/{len(train_loader)} | Loss: {running_loss/200:.4f}\"\n",
    "                    )\n",
    "                    running_loss = 0\n",
    "\n",
    "            else:\n",
    "                model.train()\n",
    "\n",
    "                # TODO: receive data from rank 0\n",
    "                data_local = comm.recv(source=0, tag=0)\n",
    "                target_local = comm.recv(source=0, tag=1)\n",
    "\n",
    "\n",
    "                # TODO: move data to device\n",
    "                data_local = data_local.to(device)\n",
    "                target_local = target_local.to(device)\n",
    "\n",
    "\n",
    "                # TODO: compute loss\n",
    "                output = model(data_local)\n",
    "                loss = criterion(output, target_local)\n",
    "\n",
    "\n",
    "                # TODO: compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "\n",
    "                # TODO: send loss to rank 0, this is not necessary but just for debugging\n",
    "                # remember to multiply loss by number of local samples\n",
    "                loss *= len(data_local)\n",
    "                comm.send(loss, dest=0, tag=2)\n",
    "\n",
    "\n",
    "                # TODO: send gradients to rank 0, remember to multiply gradients by number of local samples\n",
    "                for param in model.parameters():\n",
    "                    param.grad *= len(data_local)\n",
    "                    comm.send(param.grad, dest=0, tag=3)\n",
    "\n",
    "                # TODO: receive updated model from rank 0\n",
    "                model = comm.recv(source=0, tag=10)\n",
    "\n",
    "\n",
    "                # clear gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    # evaluate model on testset after training on parameter server\n",
    "    model.eval()\n",
    "    if rank == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target in test_loader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            accuracy = correct / total\n",
    "            logging.info(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WJ9QTsg17YX",
    "outputId": "d98a3fed-e889-446c-9544-edec2b8d235d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:root:Rank 0 is using device cuda\n",
      "INFO:root:Rank 2 is using device cuda\n",
      "INFO:root:Rank 1 is using device cuda\n",
      "INFO:root:Rank 3 is using device cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO:root:Epoch: 0 Batch: 200/500 | Loss: 2.1845\n",
      "INFO:root:Epoch: 0 Batch: 400/500 | Loss: 1.7600\n",
      "INFO:root:Epoch: 1 Batch: 200/500 | Loss: 1.5021\n",
      "INFO:root:Epoch: 1 Batch: 400/500 | Loss: 1.4261\n",
      "INFO:root:Epoch: 2 Batch: 200/500 | Loss: 1.3138\n",
      "INFO:root:Epoch: 2 Batch: 400/500 | Loss: 1.2814\n",
      "INFO:root:Test Accuracy: 0.5524\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python parameter_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EKwaR2y17YX"
   },
   "source": [
    "## Federated Learning\n",
    "\n",
    "### **TODO 3:**\n",
    "\n",
    "Use MPI to simulate federated learning with fedavg algorithm. Similar to distributed training with parameter server, assume multiple clients, each equipped with one GPU (simulated by ranks other than 0), cooperate in the training process. Each rank (excluding rank 0) will be responsible for sending modles (not gradients) to the server (rank 0), which will receive, average these models, and send them back for further updates.\n",
    "\n",
    "Split the data into independent, identically distributed (IID) by specifying `iid` in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ETxCU85R17YX",
    "outputId": "1d405251-174b-460d-8255-5a1b5115b10b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing fed_avg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fed_avg.py\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from utils import load_data, create_dataloader, set_random_seed, Net, data_split, plot_data_split\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def get_model_params(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def set_model_params(self, model_params):\n",
    "        self.model.load_state_dict(model_params)\n",
    "\n",
    "    def aggregate(self, state_dict_list, num_samples_list):\n",
    "        # TODO: aggregate local model params using federated averaging algorithm\n",
    "        # the weight of each local model is num of samples on that client / total num of samples\n",
    "        # return the aggregated model params in the form of a state_dict\n",
    "        avg_params = state_dict_list[0]\n",
    "        for k in avg_params.keys():\n",
    "            for i in range(0, len(state_dict_list)):\n",
    "                local_model_params = state_dict_list[i]\n",
    "                w = num_samples_list[i] / sum(num_samples_list)\n",
    "                if i == 0:\n",
    "                    avg_params[k] = local_model_params[k] * w\n",
    "                else:\n",
    "                    avg_params[k] += local_model_params[k] * w\n",
    "\n",
    "        return avg_params\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # fix randomness\n",
    "    set_random_seed(42)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Rank {rank} is using device {device}\")\n",
    "\n",
    "    # define model and move to device\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # define optimizer and loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    num_rounds = 5\n",
    "    num_epochs = 1\n",
    "    batch_size = 32\n",
    "\n",
    "    # load cifar10 dataset\n",
    "    train_data, test_data = load_data()\n",
    "\n",
    "    # in simulation each client can see the whole dataset but only uses part of it\n",
    "    # however, in practice, each client will have its distinct dataset and cannot access others' dataset\n",
    "    split_method = sys.argv[1]  # options: \"non-iid\" or \"iid\"\n",
    "    if rank == 0:\n",
    "        logging.info(f\"Data split method: {split_method}\")\n",
    "\n",
    "    train_data_split = data_split(\n",
    "        dataset=train_data, num_clients=size - 1, split_method=split_method\n",
    "    )\n",
    "\n",
    "    # plot the distribution of each client's dataset\n",
    "    plot_data_split(\n",
    "        data_split=train_data_split,\n",
    "        num_clients=size - 1,\n",
    "        num_classes=10,\n",
    "        file_name=f\"./{split_method}.png\",\n",
    "    )\n",
    "\n",
    "    test_data_split = torch.utils.data.random_split(\n",
    "        test_data,\n",
    "        [len(test_data) // (size - 1) for _ in range(size - 1)],\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        # initialize aggregator in the parameter server\n",
    "        aggregator = Aggregator(model)\n",
    "\n",
    "    # start federated learning\n",
    "    for round_idx in range(num_rounds):\n",
    "        # rank 0 is the parameter server and rest of the ranks are clients\n",
    "        if rank == 0:\n",
    "            # TODO: get global model params from the aggregator\n",
    "            global_model_params = aggregator.get_model_params()\n",
    "\n",
    "            # TODO: send global model params to other ranks\n",
    "            for i in range(1, size):\n",
    "                comm.send(global_model_params, dest=i, tag=0)\n",
    "\n",
    "            local_model_params_list = []\n",
    "            num_samples_list = []\n",
    "            # TODO: receive local model params from other ranks and append to local_model_params_list\n",
    "            # and receive number of samples from other ranks and append to num_samples_list\n",
    "            for i in range(1, size):\n",
    "                local_model_params_list.append(comm.recv(source=i, tag=1))\n",
    "                num_samples_list.append(comm.recv(source=i, tag=2))\n",
    "\n",
    "            # TODO: aggregate local model params\n",
    "            global_model_params = aggregator.aggregate(local_model_params_list, num_samples_list)\n",
    "\n",
    "            # TODO: set global model params to the aggregator\n",
    "            aggregator.set_model_params(global_model_params)\n",
    "\n",
    "\n",
    "            logging.info(\n",
    "                f\"--------- | Round: {round_idx} | aggregation finished | ---------\"\n",
    "            )\n",
    "        else:\n",
    "            # TODO: create data loader on local dataset\n",
    "            train_loader, test_loader = create_dataloader(\n",
    "                train_data_split[rank - 1],\n",
    "                test_data_split[rank - 1],\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "            # TODO: receive global model params from parameter server\n",
    "            global_model_params = comm.recv(source=0, tag=0)\n",
    "\n",
    "            # TODO: set local model params\n",
    "            model.load_state_dict(global_model_params)\n",
    "\n",
    "            # start local training\n",
    "            model.train()\n",
    "            running_loss = 0\n",
    "\n",
    "            for epoch_idx in range(num_epochs):\n",
    "                for batch_idx, (data_local, target_local) in enumerate(train_loader):\n",
    "                    data_local = data_local.to(device)\n",
    "                    target_local = target_local.to(device)\n",
    "\n",
    "                    output = model(data_local)\n",
    "                    loss = criterion(output, target_local)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "            # evaluate model on testset\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data_local, target_local in test_loader:\n",
    "                    data_local = data_local.to(device)\n",
    "                    target_local = target_local.to(device)\n",
    "\n",
    "                    output = model(data_local)\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    total += target_local.size(0)\n",
    "                    correct += (predicted == target_local).sum().item()\n",
    "\n",
    "                accuracy = correct / total\n",
    "                running_loss = running_loss / len(train_loader) / num_epochs\n",
    "                logging.info(\n",
    "                    f\"Client: {rank} | Round: {round_idx} | Loss: {running_loss:.4f} | Accuracy: {accuracy:.4f}\"\n",
    "                )\n",
    "\n",
    "            # TODO: send local model params to parameter server\n",
    "            local_model_params = model.state_dict()\n",
    "            comm.send(local_model_params, dest=0, tag=1)\n",
    "\n",
    "            # TODO: send number of samples to parameter server\n",
    "            num_samples = len(train_data_split[rank - 1])\n",
    "            comm.send(num_samples, dest=0, tag=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "n0GGnsQd17YY",
    "outputId": "3dbce4ae-2a04-431a-f906-6ce00b28b532",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:root:Rank 0 is using device cuda\n",
      "INFO:root:Rank 4 is using device cuda\n",
      "INFO:root:Rank 3 is using device cuda\n",
      "INFO:root:Rank 2 is using device cuda\n",
      "INFO:root:Rank 1 is using device cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO:root:Data split method: iid\n",
      "INFO:root:Client: 4 | Round: 0 | Loss: 2.0618 | Accuracy: 0.3596\n",
      "INFO:root:Client: 1 | Round: 0 | Loss: 2.0588 | Accuracy: 0.3312\n",
      "INFO:root:Client: 3 | Round: 0 | Loss: 2.0589 | Accuracy: 0.3628\n",
      "INFO:root:Client: 2 | Round: 0 | Loss: 2.0660 | Accuracy: 0.3232\n",
      "INFO:root:--------- | Round: 0 | aggregation finished | ---------\n",
      "INFO:root:Client: 2 | Round: 1 | Loss: 1.6750 | Accuracy: 0.4268\n",
      "INFO:root:Client: 3 | Round: 1 | Loss: 1.6782 | Accuracy: 0.4252\n",
      "INFO:root:Client: 4 | Round: 1 | Loss: 1.6486 | Accuracy: 0.4452\n",
      "INFO:root:Client: 1 | Round: 1 | Loss: 1.6566 | Accuracy: 0.4164\n",
      "INFO:root:--------- | Round: 1 | aggregation finished | ---------\n",
      "INFO:root:Client: 1 | Round: 2 | Loss: 1.5112 | Accuracy: 0.4828\n",
      "INFO:root:Client: 4 | Round: 2 | Loss: 1.5292 | Accuracy: 0.4688\n",
      "INFO:root:Client: 3 | Round: 2 | Loss: 1.5447 | Accuracy: 0.4628\n",
      "INFO:root:Client: 2 | Round: 2 | Loss: 1.5370 | Accuracy: 0.4564\n",
      "INFO:root:--------- | Round: 2 | aggregation finished | ---------\n",
      "INFO:root:Client: 2 | Round: 3 | Loss: 1.4461 | Accuracy: 0.4668\n",
      "INFO:root:Client: 3 | Round: 3 | Loss: 1.4494 | Accuracy: 0.4744\n",
      "INFO:root:Client: 4 | Round: 3 | Loss: 1.4246 | Accuracy: 0.5180\n",
      "INFO:root:Client: 1 | Round: 3 | Loss: 1.4223 | Accuracy: 0.4888\n",
      "INFO:root:--------- | Round: 3 | aggregation finished | ---------\n",
      "INFO:root:Client: 3 | Round: 4 | Loss: 1.3642 | Accuracy: 0.4968\n",
      "INFO:root:Client: 4 | Round: 4 | Loss: 1.3374 | Accuracy: 0.5212\n",
      "INFO:root:Client: 1 | Round: 4 | Loss: 1.3408 | Accuracy: 0.4780\n",
      "INFO:root:Client: 2 | Round: 4 | Loss: 1.3700 | Accuracy: 0.5100\n",
      "INFO:root:--------- | Round: 4 | aggregation finished | ---------\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 5 python fed_avg.py iid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQdq41ZG17YY"
   },
   "source": [
    "### **TODO 4:**\n",
    "\n",
    "Split the data into non-independent, identically distributed (Non-IID) by specifying `non-iid` in the command line and run the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Q8_Sg2la17YY",
    "outputId": "f0040e0d-7fde-4e06-e9d4-20d9f9e85397",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:root:Rank 1 is using device cuda\n",
      "INFO:root:Rank 3 is using device cuda\n",
      "INFO:root:Rank 2 is using device cuda\n",
      "INFO:root:Rank 4 is using device cuda\n",
      "INFO:root:Rank 0 is using device cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO:root:Data split method: non-iid\n",
      "INFO:root:Client: 3 | Round: 0 | Loss: 0.5874 | Accuracy: 0.1012\n",
      "INFO:root:Client: 1 | Round: 0 | Loss: 1.0305 | Accuracy: 0.2056\n",
      "INFO:root:Client: 4 | Round: 0 | Loss: 0.8795 | Accuracy: 0.2452\n",
      "INFO:root:Client: 2 | Round: 0 | Loss: 0.9865 | Accuracy: 0.2500\n",
      "INFO:root:--------- | Round: 0 | aggregation finished | ---------\n",
      "INFO:root:Client: 3 | Round: 1 | Loss: 0.4831 | Accuracy: 0.1032\n",
      "INFO:root:Client: 2 | Round: 1 | Loss: 0.7731 | Accuracy: 0.2484\n",
      "INFO:root:Client: 1 | Round: 1 | Loss: 0.8425 | Accuracy: 0.2076\n",
      "INFO:root:Client: 4 | Round: 1 | Loss: 0.6171 | Accuracy: 0.2452\n",
      "INFO:root:--------- | Round: 1 | aggregation finished | ---------\n",
      "INFO:root:Client: 3 | Round: 2 | Loss: 0.4564 | Accuracy: 0.1720\n",
      "INFO:root:Client: 1 | Round: 2 | Loss: 0.7759 | Accuracy: 0.2160\n",
      "INFO:root:Client: 4 | Round: 2 | Loss: 0.5379 | Accuracy: 0.2528\n",
      "INFO:root:Client: 2 | Round: 2 | Loss: 0.6975 | Accuracy: 0.2552\n",
      "INFO:root:--------- | Round: 2 | aggregation finished | ---------\n",
      "INFO:root:Client: 3 | Round: 3 | Loss: 0.3995 | Accuracy: 0.2196\n",
      "INFO:root:Client: 1 | Round: 3 | Loss: 0.7527 | Accuracy: 0.2160\n",
      "INFO:root:Client: 2 | Round: 3 | Loss: 0.6535 | Accuracy: 0.2924\n",
      "INFO:root:Client: 4 | Round: 3 | Loss: 0.4989 | Accuracy: 0.2648\n",
      "INFO:root:--------- | Round: 3 | aggregation finished | ---------\n",
      "INFO:root:Client: 3 | Round: 4 | Loss: 0.3766 | Accuracy: 0.1660\n",
      "INFO:root:Client: 1 | Round: 4 | Loss: 0.7148 | Accuracy: 0.2096\n",
      "INFO:root:Client: 4 | Round: 4 | Loss: 0.4617 | Accuracy: 0.2544\n",
      "INFO:root:Client: 2 | Round: 4 | Loss: 0.6158 | Accuracy: 0.2940\n",
      "INFO:root:--------- | Round: 4 | aggregation finished | ---------\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 5 python fed_avg.py non-iid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiqHlVGr17YY"
   },
   "source": [
    "### **TODO 5:**\n",
    "\n",
    "1. Check the data distribution plots named \"iid.png\" and \"non-idd.png\" by clicking the \"Files\" folder icon on the left pannel bar. Comment on why non-iid federated learning could lead to worse performance.\n",
    "1. Assume the model has `P` trainable parameters. For distributed training, we have `N` processes and train `S` steps (a step means one update of the model's parameters using a batch of training data). For federated learning, we have `N` clients and train `R` rounds. Analyze the total amount of data transmission for each paradigm and quantify it in terms of `P`, `N`, `S`, or `R`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "he0spWrW17YY"
   },
   "source": [
    "Your answer:\n",
    "\n",
    "1. In non-iid federated learning, sample data in each client is bias in particular class, leading to weight divergence during the training process.\n",
    "\n",
    "2. Distributed Training: $2PNS$ because in each step, loss is gathered, calculated and then reduced distributedly. Therefore, for each parameters in one process in one step, there are 2 data transmission.\n",
    "\n",
    "  Federated Learning: $2NR$ because in each round of the federated learning, the clients receive the model parameters from the server, and send back the trained parameters back to the server after training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

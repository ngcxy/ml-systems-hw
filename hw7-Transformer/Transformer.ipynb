{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0XOR9x6daBP"
      },
      "source": [
        "# EE 599 HW 7: Transformer\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any).\n",
        "\n",
        "Prerequisites: set the runtime type to GPU. (Runtime -> Change Runtime Type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttKcA361dgPa"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "This section imports all required packages from PyTorch and parameter initializations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "27xci-S5dp_q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import math\n",
        "\n",
        "# Define the source vocabulary size\n",
        "src_vocab_size = 500\n",
        "\n",
        "# Define the target vocabulary size\n",
        "tgt_vocab_size = 500\n",
        "\n",
        "# Define the model's dimensionality (size of embeddings and hidden layers)\n",
        "d_model = 512\n",
        "\n",
        "# Define the number of attention heads in multi-head attention layers\n",
        "num_heads = 8\n",
        "\n",
        "# Define the number of encoder and decoder layers in the transformer\n",
        "num_layers = 4\n",
        "\n",
        "# Define the dimension of the feed-forward network in each layer\n",
        "d_ff = 2048\n",
        "\n",
        "# Define the maximum sequence length for input and output sequences\n",
        "max_seq_length = 50\n",
        "\n",
        "# Define the dropout rate (0 for no dropout, otherwise specify a value)\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EiXx420dsJW"
      },
      "source": [
        "## Multi-Head Attention Block\n",
        "\n",
        "In the Transformer architecture, multi-head attention operates as follows:\n",
        "\n",
        "1. Linear Projection: Input is linearly projected into queries (Q), keys (K), and values (V) for each attention head, diversifying the data representation.\n",
        "\n",
        "2. Calculate Attention: Each head computes attention scores using the equation:\n",
        "    $$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{SoftMax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}$$\n",
        "    It scales the dot-product of Q and K, normalizes scores with softmax to obtain probabilities, and calculates a weighted sum of V, allowing each head to focus on different parts of the input.\n",
        "\n",
        "3. Combine Outputs: Outputs from all heads are concatenated and passed through a linear transformation to produce the final output, integrating the insights from each head into a unified representation.\n",
        "\n",
        "[![attention.png](https://i.postimg.cc/Jn1f9GW3/attention.png)](https://postimg.cc/wtG4NxM3)\n",
        "\n",
        "Note that the input and output linear layers in steps 1 and 3 are not depicted in the picture above.\n",
        "\n",
        "### **TODO 1:**\n",
        "Finish functions `scaled_dot_product_attention` and `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "L2ZWPH6Pf5pu"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Check if d_model is divisible by num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Store d_model and num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Calculate dimension of each head\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear transformations for queries, keys, values, and output\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # TODO: Implement the scaled dot product attention.\n",
        "        # Your task is to calculate attention scores using the dot product\n",
        "        # between Q and K, scale these scores and apply a mask if provided.\n",
        "        # Finally, calculate the output as a weighted sum of V.\n",
        "\n",
        "        # Calculate attention scores using dot product and scale by sqrt(d_k)\n",
        "        # (assuming a shape like [batch_size, sequence_length, embedding_dimension]) for K.\n",
        "        scores = torch.matmul(Q, K.transpose(-1,-2))\n",
        "\n",
        "        scores = scores / torch.sqrt(torch.tensor(self.d_k))\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = nn.Softmax(dim=-1)(scores)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # TODO: Implement the split_heads function.\n",
        "        # Your task is to reshape the input tensor x so that it is split\n",
        "        # into multiple heads for multi-head attention.\n",
        "        # Reshape input into multiple heads\n",
        "        # The final shape should be\n",
        "        # (batch_size, num_heads, seq_length, d_k)\n",
        "\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # TODO: Combine multiple heads into original shape\n",
        "        # Finale shape is (batch_size, seq_length, d_model)\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # TODO: Implement the forward pass for the MultiHeadAttention.\n",
        "        # Your task is to transform Q, K, V linearly, split them into multiple heads,\n",
        "        # apply scaled dot product attention, combine the heads back and apply\n",
        "        # the final linear transformation.\n",
        "\n",
        "        # Linearly transform queries, keys, and values for each head\n",
        "\n",
        "\n",
        "        Q_transformed = self.W_q(Q)\n",
        "        K_transformed = self.W_k(K)\n",
        "        V_transformed = self.W_v(V)\n",
        "\n",
        "        Q_split = self.split_heads(Q_transformed)\n",
        "        K_split = self.split_heads(K_transformed)\n",
        "        V_split = self.split_heads(V_transformed)\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask)\n",
        "\n",
        "        attn_output_combined = self.combine_heads(attn_output)\n",
        "\n",
        "        output = self.W_o(attn_output_combined)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XertxnKhnk1l"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "The `PositionalEncoding` class adds positional information to input sequences using a sinusoidal function, enabling the Transformer model to consider the order of tokens. The encoding for a position `pos` and dimension `i` in the input is defined as follows:\n",
        "\n",
        "$$\n",
        "\\text{PE(pos, i)} =\n",
        "\\begin{cases}\n",
        "\\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{if i is even} \\\\\n",
        "\\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{if i is odd}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where\n",
        "* `pos` represents the position of the token in the sequence.\n",
        "* `i` refers to the dimension within the embedding space.\n",
        "* `d_model` is the total size of the embeddings.\n",
        "\n",
        "[![positional-encoding.png](https://i.postimg.cc/xjpzkxnv/positional-encoding.png)](https://postimg.cc/bdbJWRGr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tWQZb0UwnpVh"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Initialize a matrix 'pe' to store positional encodings.\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Create a 'position' tensor to represent positions from 0 to 'max_seq_length - 1'.\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Calculate the 'div_term' tensor to scale the sine and cosine functions.\n",
        "        # Note that a^b = exp(b * log(a))\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        # Calculate sine and cosine positional encodings for each position and dimension.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register 'pe' as a buffer so it's not considered a model parameter.\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add the positional encodings to the input tensor 'x'.\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEw03J8MmHqz"
      },
      "source": [
        "## Position-wise Feed Forward Block\n",
        "\n",
        "The Position-Wise Feed-Forward Network (FFN) consists of two fully connected dense layers, or a multi-layer perceptron (MLP). The hidden layer, which is known as `d_ffn`, is generally set to a value about four times that of `d_model`. This is why it is sometimes known as an expand-and-contract network.\n",
        "\n",
        "It is known as a “position-wise” network because it “transforms the representation at all the sequence positions using the same MLP.” In other words, the FNN has a size of (`d_model`, `d_ffn`) in its first layer, which means it has to be broadcast across each sequence during tensor multiplication. This means each sequence is multiplied by the same weights. If identical sequences are input, the outputs will also be identical. This same logic applies to the second dense layer of size (`d_ffn`, `d_model`), which returns the tensor to its original size.\n",
        "\n",
        "The ReLU activiation function is used between the layers. Any values greater than 0 remain the same, and any values less than or equal to 0 become 0. It introduces non-linearity and helps prevent vanishing gradients.\n",
        "\n",
        "### **TODO 2:**\n",
        "Finish the function `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WEJdhLETmW6d"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "        # First linear layer (input dimension to intermediate dimension)\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "\n",
        "        # Second linear layer (intermediate dimension to output dimension)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        # ReLU activation function to introduce non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass for the PositionWiseFeedForward.\n",
        "        # Your task is to apply the first linear layer (self.fc1) to the input x,\n",
        "        # then apply a ReLU activation function, followed by applying the second\n",
        "        # linear layer (self.fc2). The goal is to transform the input and then\n",
        "        # map it back to the original dimension.\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89z8KU-So5dV"
      },
      "source": [
        "## Encoder Layer\n",
        "\n",
        "The `EncoderLayer` represents one layer of the encoder stack in the Transformer model. It takes an input tensor `x` and a `mask` (used in the self-attention mechanism), applies multi-head self-attention, position-wise feedforward operations, layer normalization, and dropout.\n",
        "\n",
        "The layer normalization and dropout are used to enhance the model's stability and prevent overfitting.\n",
        "\n",
        "[![encoder.png](https://i.postimg.cc/MpDynwD9/encoder.png)](https://postimg.cc/TKKysZBb)\n",
        "\n",
        "### **TODO 3:**\n",
        "Finish the function `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TxR0uBQZo7td"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Position-wise feedforward network\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization after self-attention and feedforward\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # TODO: Implement the forward pass for the EncoderLayer.\n",
        "        # Your task is to apply multi-head self-attention on the input x using\n",
        "        # the self.self_attn layer with a residual connection and layer normalization.\n",
        "        # Next, apply the position-wise feedforward network using self.feed_forward\n",
        "        # with another residual connection and layer normalization.\n",
        "        # Remember to include dropout for regularization.\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout(ff_output)\n",
        "        x = self.norm2(x + ff_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH2PakIzsaFi"
      },
      "source": [
        "## Decoder Layer\n",
        "\n",
        "The `DecoderLayer` represents one layer of the decoder stack in the Transformer model. It takes an input tensor `x`, the encoder's output `enc_output`, and masks for both the source (`src_mask`) and target (`tgt_mask`) sequences.\n",
        "\n",
        "The layer performs self-attention, cross-attention, and feedforward operations, followed by layer normalization and dropout for regularization.\n",
        "\n",
        "The residual connections ensure that information from previous layers is preserved, and layer normalization stabilizes the activations.\n",
        "\n",
        "[![decoder.png](https://i.postimg.cc/xdbJQScx/decoder.png)](https://postimg.cc/t1pCdwdW)\n",
        "\n",
        "### **TODO 4:**\n",
        "Finish the function `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cxuInnMbsax2"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Self-attention layer within the decoder\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Cross-attention layer between decoder and encoder\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Position-wise feedforward network\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization after each sub-layer\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        # TODO: Implement the forward pass for the DecoderLayer.\n",
        "        # Your task is to apply self-attention on the input x using self.self_attn\n",
        "        # with a residual connection and layer normalization. Next, apply cross-attention\n",
        "        # using self.cross_attn, with the encoder output as key and value, and a residual\n",
        "        # connection and layer normalization. Finally, apply the position-wise feedforward\n",
        "        # network using self.feed_forward with another residual connection and layer normalization.\n",
        "        # Remember to use dropout for regularization where appropriate.\n",
        "\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        self_attn_output = self.dropout(self_attn_output)\n",
        "        x = self.norm1(x + self_attn_output)\n",
        "\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        cross_attn_output = self.dropout(cross_attn_output)\n",
        "        x = self.norm2(x + cross_attn_output)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout(ff_output)\n",
        "        x = self.norm3(x + ff_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XnR3H2kuCyV"
      },
      "source": [
        "## The Transformer Class\n",
        "\n",
        "In this class we put together the encoder and decoder layers.\n",
        "\n",
        "Note that we define two types of masks, one for encoder and one for decoder:\n",
        "\n",
        "1. Source Mask (`src_mask`): The source mask is applied to the source sequence (usually the input sequence). It is used to mask positions in the source sequence where padding tokens are present. Padding tokens are often used in sequences with varying lengths, and the mask ensures that the model does not attend to or consider the padding positions during processing.\n",
        "\n",
        "2. Target Mask (`tgt_mask`): The target mask is applied to the target sequence (usually the output or target sequence). It serves two purposes:\n",
        "    * First, it masks positions in the target sequence where padding tokens are present, similar to the source mask.\n",
        "    * Second, it includes a \"no-peek\" mask, which is used to prevent the decoder from looking ahead during sequence generation. This is achieved by creating a triangular mask with ones below the main diagonal and zeros above the diagonal using torch.triu. This mask ensures that each position in the target sequence can only attend to or consider positions that precede it, maintaining the causal nature of sequence generation. The no-peek mask is applied after the padding mask.\n",
        "\n",
        "[![transformer.png](https://i.postimg.cc/sXYBLd2k/transformer.png)](https://postimg.cc/k65XVHKs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "01zie-b_u_ly"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Embedding layers for the source and target sequences\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding for both source and target sequences\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Stacks of encoder and decoder layers\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # Fully connected layer for output prediction\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        # Generate masks for the source and target sequences\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "\n",
        "        # Create a no-peek mask to prevent the decoder from looking ahead\n",
        "        nopeek_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeek_mask.to(tgt_mask.device)\n",
        "\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # TODO: Implement the forward pass for the Transformer.\n",
        "        # Your task is to generate masks for the source and target sequences.\n",
        "        # Then embed the source and target sequences, apply positional encoding,\n",
        "        # and pass them through the stacks of encoder and decoder layers.\n",
        "        # Finally, apply the fully connected layer to get the output prediction.\n",
        "        # Remember to use dropout where appropriate.\n",
        "\n",
        "        # Generate masks for source and target sequences\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        # Embed source and target sequences and apply positional encoding\n",
        "        # Apply dropout to the embeddings\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        # Encoder: Apply multiple encoder layers to the source sequence\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        # Decoder: Apply multiple decoder layers to the target sequence\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Fully connected layer for output prediction\n",
        "        output = self.fc(dec_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMlaaRt0xfMO"
      },
      "source": [
        "## Create A Simple Word Tokenizer\n",
        "\n",
        "The SimpleWordTokenizer class is responsible for tokenizing and encoding sentences into sequences of token IDs and decoding sequences of token IDs back into human-readable sentences.\n",
        "\n",
        "The tokenization process in the SimpleWordTokenizer class involves several steps.\n",
        "\n",
        "-  First, it converts the input sentence to lowercase to ensure uniformity.\n",
        "-  Then, it employs regular expressions to tokenize the sentence, distinguishing between words and punctuation. It uses a regular expression pattern to capture words, including accented characters in languages like Spanish and English, and also captures punctuation marks.\n",
        "- The tokenizer adds special start-of-sequence (\\<sos\\>) and end-of-sequence (\\<eos\\>) tokens to mark the beginning and end of the sequence. These tokens, along with the tokens obtained from the regular expression, are organized into a list.\n",
        "- Each token is then mapped to its corresponding index in the vocabulary, with unknown words assigned index 0. This resulting list of token IDs represents the tokenized and encoded form of the input sentence.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "# Example usage\n",
        "vocabulary = [\"hello\", \"hola\", \",\", \"how\", \"cómo\", \"are\", \"estás\", \"you\", \"tú\", \"?\", \"¡\", \"¿\"]\n",
        "tokenizer = SimpleWordTokenizer(vocabulary)\n",
        "\n",
        "encoded_sentence = tokenizer.encode(\"¿Hola, cómo estás tú?\")\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
        "print(\"Encoded:\", encoded_sentence)\n",
        "print(\"Decoded:\", decoded_sentence)\n",
        "\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "Encoded: [1, 3, 4, 5, 6, 7, 8, 2]\n",
        "Decoded: <sos> hello , how are you ? <eos>\n",
        "```\n",
        "\n",
        "Another example:\n",
        "\n",
        "```python\n",
        "# Example usage\n",
        "vocabulary = [\"hello\", \",\", \"how\", \"are\", \"you\", \"?\"]\n",
        "tokenizer = SimpleWordTokenizer(vocabulary)\n",
        "\n",
        "encoded_sentence = tokenizer.encode(\"hello, how are you?\")\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
        "print(\"Encoded:\", encoded_sentence)\n",
        "print(\"Decoded:\", decoded_sentence)\n",
        "```\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "Encoded: [1, 14, 4, 5, 7, 9, 11, 12, 2]\n",
        "Decoded: <sos> ¿ hola , cómo estás tú ? <eos>\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "logYdnle4H2o",
        "outputId": "6b82bc76-c2c5-444d-c91e-c83a6280769f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [1, 14, 4, 5, 7, 9, 11, 12, 2]\n",
            "Decoded: <sos> ¿ hola , cómo estás tú ? <eos>\n",
            "Encoded: [1, 3, 4, 5, 6, 7, 8, 2]\n",
            "Decoded: <sos> hello , how are you ? <eos>\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class SimpleWordTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        # Add SOS and EOS tokens to the vocabulary\n",
        "        self.SOS_TOKEN = \"<sos>\"  # Start-of-sequence token\n",
        "        self.EOS_TOKEN = \"<eos>\"  # End-of-sequence token\n",
        "\n",
        "        # Extend the vocabulary with SOS and EOS tokens\n",
        "        extended_vocab = [self.SOS_TOKEN, self.EOS_TOKEN] + vocabulary\n",
        "\n",
        "        # Check if the provided vocabulary is valid\n",
        "        if not extended_vocab or not isinstance(extended_vocab, list):\n",
        "            raise ValueError(\"Vocabulary must be a non-empty list of words.\")\n",
        "\n",
        "        # Create word-to-index and index-to-word dictionaries\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(extended_vocab, start=1)}  # Start at 1 to reserve 0 for padding\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        # TODO: Implement the encode method for the SimpleWordTokenizer.\n",
        "        # Your task is to convert the input sentence to lowercase,\n",
        "        # tokenize the sentence into words and punctuation using regular expressions,\n",
        "        # and then convert each token to its corresponding index in the vocabulary.\n",
        "        # If a token is not in the vocabulary, use 0 to represent it.\n",
        "\n",
        "        # Convert the input sentence to lowercase\n",
        "        sentence = sentence.lower()\n",
        "        # Enhanced tokenization: split words and punctuation using regular expression\n",
        "        # Regex pattern to capture words (including accented characters in Spanish and English) and punctuation\n",
        "        tokens = [self.SOS_TOKEN] + re.findall(r\"[\\wáéíóúüñÁÉÍÓÚÜÑ]+|[^\\w\\s]\", sentence, re.UNICODE) + [self.EOS_TOKEN]\n",
        "        return [self.word2idx.get(token, 0) for token in tokens]  # 0 for unknown words\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        # TODO: Implement the decode method for the SimpleWordTokenizer.\n",
        "        # Your task is to convert a list of token IDs back into a readable sentence.\n",
        "        # You should convert each token ID to its corresponding word in the vocabulary.\n",
        "        # If a token ID is not in the vocabulary, use \"<unk>\" to represent it.\n",
        "        # Additionally, decide how to handle the special tokens like <sos> and <eos>.\n",
        "\n",
        "        # Decode each token ID to a word\n",
        "        decoded_tokens = []\n",
        "        for token_id in token_ids:\n",
        "            # if token_id == self.word2idx[self.SOS_TOKEN] or token_id == self.word2idx[self.EOS_TOKEN]:\n",
        "            #     # Skip <sos> and <eos> tokens in the decoded output\n",
        "            #     continue\n",
        "            decoded_tokens.append(self.idx2word.get(token_id, \"<unk>\"))  # Use <unk> for unknown words\n",
        "        return ' '.join(decoded_tokens)\n",
        "\n",
        "# Example usage\n",
        "vocabulary = [\"hello\", \"hola\", \",\", \"how\", \"cómo\", \"are\", \"estás\", \"you\", \"tú\", \"?\", \"¡\", \"¿\"]\n",
        "tokenizer = SimpleWordTokenizer(vocabulary)\n",
        "\n",
        "encoded_sentence = tokenizer.encode(\"¿Hola, cómo estás tú?\")\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
        "print(\"Encoded:\", encoded_sentence)\n",
        "print(\"Decoded:\", decoded_sentence)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "vocabulary = [\"hello\", \",\", \"how\", \"are\", \"you\", \"?\"]\n",
        "tokenizer = SimpleWordTokenizer(vocabulary)\n",
        "\n",
        "encoded_sentence = tokenizer.encode(\"hello, how are you?\")\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
        "print(\"Encoded:\", encoded_sentence)\n",
        "print(\"Decoded:\", decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrXGmsFyyoRo"
      },
      "source": [
        "## Padding Function\n",
        "\n",
        "The `pad_sequences` function takes a list of variable-length sequences and pads them to a specified maximum length, resulting in a batch of sequences with consistent lengths.\n",
        "\n",
        "Padding is achieved using zeros, and the function returns the padded sequences as PyTorch tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KgrSAYAPyrLh"
      },
      "outputs": [],
      "source": [
        "# Padding function\n",
        "def pad_sequences(sequences, max_len):\n",
        "    # TODO: Implement the pad_sequences function.\n",
        "    # Your task is to convert each sequence in the list to a PyTorch tensor,\n",
        "    # pad these sequences to have a consistent length using torch.nn.utils.rnn.pad_sequence,\n",
        "    # ensure that the length of each sequence does not exceed 'max_len',\n",
        "    # and optionally convert the result to int64 data type.\n",
        "    # Remember to handle the padding value appropriately.\n",
        "\n",
        "    # Convert each sequence in the list to a PyTorch tensor\n",
        "    tensor_sequences = [torch.tensor(seq) for seq in sequences]\n",
        "\n",
        "    # Pad the sequences to have a consistent length using torch.nn.utils.rnn.pad_sequence\n",
        "    padded_sequences = torch.nn.utils.rnn.pad_sequence(tensor_sequences, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Ensure the padded sequences have a maximum length of 'max_len' by slicing\n",
        "    padded_sequences = padded_sequences[:, :max_len]\n",
        "\n",
        "    # Convert the result to int64 data type (optional, depending on your needs)\n",
        "    padded_sequences = padded_sequences.to(torch.int64)\n",
        "\n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_neJYm9tzUwL"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "In this section, we should use a Bilingual dataset from a target language to a source language.\n",
        "\n",
        "We prepare English and Spanish sentences for sequence-to-sequence tasks by creating vocabularies, tokenizing and encoding the sentences, and padding the sequences to ensure consistent sequence lengths for further processing.\n",
        "\n",
        "Ideally we should use an open source dataset, but for simplicity, we just use a minimal dataset here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S4hqsZYG7U_n"
      },
      "outputs": [],
      "source": [
        "english_sentences=[\n",
        "    \"Hello, how are you?\",  # English\n",
        "    \"What is your name?\",\n",
        "    \"I am learning Spanish.\",\n",
        "    \"This is a book.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Where is the nearest restaurant?\",\n",
        "    \"Can you help me?\",\n",
        "    \"I love reading books.\",\n",
        "    \"How much does this cost?\",\n",
        "    \"I need to call a doctor.\",\n",
        "    \"What time is it?\",\n",
        "    \"Can I order this online?\",\n",
        "    \"The weather is very nice today.\",\n",
        "    \"I am going to the supermarket.\",\n",
        "    \"Do you speak English?\",\n",
        "    # Add more sentences here...\n",
        "]\n",
        "\n",
        "spanish_sentences=[\n",
        "    \"Hola, ¿cómo estás?\",  # Spanish\n",
        "    \"¿Cómo te llamas?\",\n",
        "    \"Estoy aprendiendo español.\",\n",
        "    \"Este es un libro.\",\n",
        "    \"El clima está agradable hoy.\",\n",
        "    \"¿Dónde está el restaurante más cercano?\",\n",
        "    \"¿Puedes ayudarme?\",\n",
        "    \"Me encanta leer libros.\",\n",
        "    \"¿Cuánto cuesta esto?\",\n",
        "    \"Necesito llamar a un médico.\",\n",
        "    \"¿Qué hora es?\",\n",
        "    \"¿Puedo pedir esto en línea?\",\n",
        "    \"El clima está muy agradable hoy.\",\n",
        "    \"Voy al supermercado.\",\n",
        "    \"¿Hablas inglés?\",\n",
        "    # Add more sentences here...\n",
        "]\n",
        "\n",
        "\n",
        "english_sentences.extend([\n",
        "    \"I enjoy traveling.\",\n",
        "    \"Where is the nearest airport?\",\n",
        "    \"What's your favorite movie?\",\n",
        "    \"Do you have any siblings?\",\n",
        "    \"Can you recommend a good restaurant?\",\n",
        "    \"I like to play soccer.\",\n",
        "    \"How do I get to the museum?\",\n",
        "    \"I'm feeling tired today.\",\n",
        "    \"What's the weather like tomorrow?\",\n",
        "    \"I have a meeting at 2 PM.\",\n",
        "    \"How was your weekend?\",\n",
        "    \"What's your favorite book?\",\n",
        "    \"I need to buy some groceries.\",\n",
        "    \"I'm going to the gym later.\",\n",
        "    \"Do you have any pets?\",\n",
        "    # Add more sentences here...\n",
        "])\n",
        "\n",
        "spanish_sentences.extend([\n",
        "    \"Disfruto viajar.\",\n",
        "    \"¿Dónde está el aeropuerto más cercano?\",\n",
        "    \"¿Cuál es tu película favorita?\",\n",
        "    \"¿Tienes hermanos?\",\n",
        "    \"¿Puedes recomendar un buen restaurante?\",\n",
        "    \"Me gusta jugar al fútbol.\",\n",
        "    \"¿Cómo llego al museo?\",\n",
        "    \"Hoy me siento cansado.\",\n",
        "    \"¿Cómo estará el clima mañana?\",\n",
        "    \"Tengo una reunión a las 2 PM.\",\n",
        "    \"¿Cómo fue tu fin de semana?\",\n",
        "    \"¿Cuál es tu libro favorito?\",\n",
        "    \"Necesito comprar algunos víveres.\",\n",
        "    \"Voy al gimnasio más tarde.\",\n",
        "    \"¿Tienes mascotas?\",\n",
        "    # Add more sentences here...\n",
        "])\n",
        "\n",
        "english_sentences.extend([\n",
        "    \"What's your favorite color?\",\n",
        "    \"I enjoy hiking in the mountains.\",\n",
        "    \"Have you visited Paris before?\",\n",
        "    \"Could you please pass the salt?\",\n",
        "    \"I'm planning a trip to Italy.\",\n",
        "    \"What's the capital of Spain?\",\n",
        "    \"I love listening to music.\",\n",
        "    \"Is there a pharmacy nearby?\",\n",
        "    \"Do you like cooking?\",\n",
        "    \"I'll be there in 10 minutes.\",\n",
        "    \"What's your favorite type of cuisine?\",\n",
        "    \"How do you say 'hello' in Spanish?\",\n",
        "    \"I need to buy a birthday gift.\",\n",
        "    \"I can speak three languages.\",\n",
        "    \"Let's go for a walk in the park.\",\n",
        "    # Add more sentences here...\n",
        "])\n",
        "\n",
        "spanish_sentences.extend([\n",
        "    \"¿Cuál es tu color favorito?\",\n",
        "    \"Disfruto de hacer senderismo en las montañas.\",\n",
        "    \"¿Has visitado París antes?\",\n",
        "    \"¿Podrías pasarme la sal, por favor?\",\n",
        "    \"Estoy planeando un viaje a Italia.\",\n",
        "    \"¿Cuál es la capital de España?\",\n",
        "    \"Me encanta escuchar música.\",\n",
        "    \"¿Hay una farmacia cerca?\",\n",
        "    \"¿Te gusta cocinar?\",\n",
        "    \"Estaré allí en 10 minutos.\",\n",
        "    \"¿Cuál es tu tipo de cocina favorito?\",\n",
        "    \"¿Cómo se dice 'hola' en español?\",\n",
        "    \"Necesito comprar un regalo de cumpleaños.\",\n",
        "    \"Puedo hablar tres idiomas.\",\n",
        "    \"Vamos a dar un paseo en el parque.\",\n",
        "    # Add more sentences here...\n",
        "])\n",
        "\n",
        "english_sentences.extend([\n",
        "    \"I just finished reading a fascinating book.\",\n",
        "    \"What's your all-time favorite book?\",\n",
        "    \"I can't put this book down; it's so captivating.\",\n",
        "    \"Do you prefer reading e-books or physical books?\",\n",
        "    \"I'm looking for a good mystery book to read.\",\n",
        "    \"Reading a book is a great way to relax.\",\n",
        "    \"I've heard great reviews about that book.\",\n",
        "    \"Can you recommend a book for my book club?\",\n",
        "    \"This bookstore has a wide selection of books.\",\n",
        "    \"I love the smell of old books in a library.\",\n",
        "    \"I'm trying to find a rare and collectible book.\",\n",
        "    \"The author of this book is a renowned writer.\",\n",
        "    \"I often visit the local library to borrow books.\",\n",
        "    \"I enjoy discussing books with fellow readers.\",\n",
        "    \"There's nothing like getting lost in a good book.\",\n",
        "    # Add more sentences here...\n",
        "])\n",
        "\n",
        "spanish_sentences.extend([\n",
        "    \"Acabo de terminar de leer un libro fascinante.\",\n",
        "    \"¿Cuál es tu libro favorito de todos los tiempos?\",\n",
        "    \"No puedo dejar este libro; es tan cautivador.\",\n",
        "    \"¿Prefieres leer libros electrónicos o libros físicos?\",\n",
        "    \"Estoy buscando un buen libro de misterio para leer.\",\n",
        "    \"Leer un libro es una excelente forma de relajarse.\",\n",
        "    \"He escuchado excelentes críticas sobre ese libro.\",\n",
        "    \"¿Puedes recomendarme un libro para mi club de lectura?\",\n",
        "    \"Esta librería tiene una amplia selección de libros.\",\n",
        "    \"Me encanta el olor de los libros antiguos en una biblioteca.\",\n",
        "    \"Estoy tratando de encontrar un libro raro y coleccionable.\",\n",
        "    \"El autor de este libro es un escritor de renombre.\",\n",
        "    \"A menudo visito la biblioteca local para tomar libros prestados.\",\n",
        "    \"Disfruto discutiendo libros con otros lectores.\",\n",
        "    \"No hay nada como perderse en un buen libro.\",\n",
        "    # Add more sentences here...\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to create vocabularies from sentences\n",
        "def create_vocabulary(sentences):\n",
        "    vocabulary = set()\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        words = re.findall(r\"[\\wáéíóúüñÁÉÍÓÚÜÑ]+|[^\\w\\s]\", sentence, re.UNICODE)   # Simple split; consider better tokenization in real applications\n",
        "        vocabulary.update(words)\n",
        "    return list(vocabulary)\n",
        "\n",
        "# Create vocabularies from the sentences\n",
        "english_vocab = create_vocabulary(english_sentences)\n",
        "spanish_vocab = create_vocabulary(spanish_sentences)\n",
        "\n",
        "# Initialize tokenizers with created vocabularies\n",
        "english_tokenizer = SimpleWordTokenizer(english_vocab)\n",
        "spanish_tokenizer = SimpleWordTokenizer(spanish_vocab)\n",
        "\n",
        "# Tokenize and encode the sentences using the tokenizers\n",
        "src_data_encoded = [english_tokenizer.encode(sentence) for sentence in english_sentences]\n",
        "tgt_data_encoded = [spanish_tokenizer.encode(sentence) for sentence in spanish_sentences]\n",
        "\n",
        "# Pad the sequences to a maximum sequence length\n",
        "src_data_padded = pad_sequences(src_data_encoded, max_seq_length)\n",
        "tgt_data_padded = pad_sequences(tgt_data_encoded, max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq_mhN1tz9GV"
      },
      "source": [
        "## Training the Transformer\n",
        "\n",
        "Train the Transformer model using the provided data (source and target sequences) with the specified hyperparameters and loss function.\n",
        "\n",
        "The model's parameters are updated iteratively through backpropagation to minimize the loss, and training progress is monitored by printing the loss at each epoch.\n",
        "\n",
        "Use `nn.CrossEntropyLoss` for the loss function and Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX0GZxpc4JDe",
        "outputId": "dd61449f-a9e5-4a46-8b39-64c49755b9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 5.54895\n",
            "Epoch: 2, Loss: 4.55114\n",
            "Epoch: 3, Loss: 4.32168\n",
            "Epoch: 4, Loss: 4.11944\n",
            "Epoch: 5, Loss: 3.98331\n",
            "Epoch: 6, Loss: 3.80535\n",
            "Epoch: 7, Loss: 3.67318\n",
            "Epoch: 8, Loss: 3.51012\n",
            "Epoch: 9, Loss: 3.33091\n",
            "Epoch: 10, Loss: 3.15608\n",
            "Epoch: 11, Loss: 3.03906\n",
            "Epoch: 12, Loss: 2.89058\n",
            "Epoch: 13, Loss: 2.77591\n",
            "Epoch: 14, Loss: 2.66065\n",
            "Epoch: 15, Loss: 2.52639\n",
            "Epoch: 16, Loss: 2.41588\n",
            "Epoch: 17, Loss: 2.30171\n",
            "Epoch: 18, Loss: 2.19261\n",
            "Epoch: 19, Loss: 2.07243\n",
            "Epoch: 20, Loss: 1.95610\n",
            "Epoch: 21, Loss: 1.85887\n",
            "Epoch: 22, Loss: 1.74130\n",
            "Epoch: 23, Loss: 1.64295\n",
            "Epoch: 24, Loss: 1.55671\n",
            "Epoch: 25, Loss: 1.45658\n",
            "Epoch: 26, Loss: 1.37709\n",
            "Epoch: 27, Loss: 1.29293\n",
            "Epoch: 28, Loss: 1.21906\n",
            "Epoch: 29, Loss: 1.12951\n",
            "Epoch: 30, Loss: 1.07030\n",
            "Epoch: 31, Loss: 0.99563\n",
            "Epoch: 32, Loss: 0.94293\n",
            "Epoch: 33, Loss: 0.88229\n",
            "Epoch: 34, Loss: 0.83117\n",
            "Epoch: 35, Loss: 0.77329\n",
            "Epoch: 36, Loss: 0.72528\n",
            "Epoch: 37, Loss: 0.66947\n",
            "Epoch: 38, Loss: 0.63140\n",
            "Epoch: 39, Loss: 0.59174\n",
            "Epoch: 40, Loss: 0.54669\n",
            "Epoch: 41, Loss: 0.51192\n",
            "Epoch: 42, Loss: 0.47134\n",
            "Epoch: 43, Loss: 0.45106\n",
            "Epoch: 44, Loss: 0.41466\n",
            "Epoch: 45, Loss: 0.38559\n",
            "Epoch: 46, Loss: 0.36546\n",
            "Epoch: 47, Loss: 0.33080\n",
            "Epoch: 48, Loss: 0.31204\n",
            "Epoch: 49, Loss: 0.28741\n",
            "Epoch: 50, Loss: 0.27150\n"
          ]
        }
      ],
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the transformer model\n",
        "transformer = Transformer(len(english_vocab)+3, len(spanish_vocab)+3, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Define the loss criterion (CrossEntropyLoss) with the 'ignore_index' option\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# Initialize the optimizer (Adam) for training the transformer\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Move the transformer model to GPU\n",
        "transformer.to(device)\n",
        "\n",
        "# Set the transformer model to training mode\n",
        "transformer.train()\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Implement the training loop for the transformer model.\n",
        "# Your task is to loop through epochs, perform a forward pass to compute the output of the transformer model,\n",
        "# calculate the loss, and perform backpropagation to update the model parameters.\n",
        "# You need to consider the shape of the output and the target data when calculating the loss.\n",
        "# Additionally, implement appropriate print statements to monitor the training progress.\n",
        "# Don't forget to zero out the gradients at the beginning of each epoch.\n",
        "# This prevents the accumulation of gradients from multiple forward passes,\n",
        "# which can lead to incorrect updates during training.\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass: Compute the output of the transformer model\n",
        "    # [:, :-1] is used to exclude the last token from each sequence\n",
        "    output = transformer(src_data_padded.to(device), tgt_data_padded[:, :-1].to(device))\n",
        "\n",
        "    # Calculate the loss by comparing the predicted output with the target sequences\n",
        "    # The '+3' accounts for special tokens: sos, eos, and padding (represented by index 0).\n",
        "\n",
        "    # 'output' is the predicted output of the Transformer model, which is a tensor of shape [batch_size, seq_length, vocab_size],\n",
        "    # where 'vocab_size' is the size of the target vocabulary, including the special tokens.\n",
        "\n",
        "    # The '.contiguous()' method is used to ensure that the tensor is stored in a contiguous block of memory.\n",
        "    # This is important for compatibility with certain operations like view() that require contiguous memory.\n",
        "\n",
        "    # '.view(-1, len(spanish_vocab)+3)' reshapes the 'output' tensor into a 2D tensor where the first dimension\n",
        "    # is automatically determined to match the size of the original tensor, and the second dimension is set to 'len(spanish_vocab)+3'.\n",
        "    # This effectively flattens the predicted output for each sequence token in the batch.\n",
        "\n",
        "    # 'tgt_data_padded[:, 1:].contiguous().view(-1)' does a similar reshaping and flattening for the target sequences.\n",
        "    # 'tgt_data_padded[:, 1:]' removes the first token from each target sequence (usually the '<sos>' token)\n",
        "    # as it's not used for loss calculation.\n",
        "\n",
        "    # The CrossEntropyLoss criterion then computes the loss by comparing the flattened predicted output\n",
        "    # with the flattened target sequences, taking into account the special tokens and padding.\n",
        "\n",
        "    loss = criterion(output.contiguous().view(-1, len(spanish_vocab)+3), tgt_data_padded[:, 1:].contiguous().view(-1).to(device))\n",
        "\n",
        "    # Backpropagation: Compute gradients and update model parameters\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the current epoch and loss for monitoring training progress\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item():.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0yXMLPW1sz9"
      },
      "source": [
        "## Inference\n",
        "\n",
        "\n",
        "In this coding Section, you will implement a [Beam Search algorithm](https://en.wikipedia.org/wiki/Beam_search) for sequence-to-sequence translation using a pre-trained Transformer model.\n",
        "\n",
        "Beam Search is a widely used decoding technique in natural language processing to generate translations or sequences of text. The goal of your code is to take a source sentence in one language, tokenize and encode it, and then generate the best possible translation in another language.\n",
        "\n",
        "You will utilize the Transformer model, which has been trained for this task, and use the beam search approach to explore multiple translation hypotheses efficiently.\n",
        "\n",
        "The provided code initializes with an initial hypothesis containing only the start-of-sentence token.\n",
        "\n",
        "You will extend this hypothesis by selecting the top-k most likely tokens at each step, where k is the beam width.\n",
        "\n",
        "You'll sort and prune hypotheses based on their log probabilities and ultimately return the best translation.\n",
        "\n",
        "Be sure to handle special tokens such as the start-of-sentence and end-of-sentence tokens appropriately.\n",
        "\n",
        "Also, ensure that your code operates efficiently and adheres to best practices in PyTorch for inference with a pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68Nuf6dL187E",
        "outputId": "49cc098d-d076-43f9-b962-21a3e38c0072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> ¿ cuál es tu color favorito ? <eos>\n",
            "<sos> leer un libro es una excelente forma de relajarse . <eos>\n"
          ]
        }
      ],
      "source": [
        "def beam_search(model, sentence, src_tokenizer, tgt_tokenizer, max_length=20, sos_token_id=1, eos_token_id=2, beam_width=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize and encode the input sentence using src_tokenizer\n",
        "    src_tokens = src_tokenizer.encode(sentence)\n",
        "    src_tensor = torch.tensor([src_tokens]).to(torch.int64).to(device)  # Add batch dimension\n",
        "\n",
        "    # Initialize the initial hypothesis with the start of sentence token\n",
        "    initial_hypothesis = [(sos_token_id, [sos_token_id], 0)]  # (current_token, sequence, log_probability)\n",
        "\n",
        "    # Initialize a list to store completed translations\n",
        "    completed_translations = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        new_hypotheses = []\n",
        "\n",
        "        for token, sequence, log_prob in initial_hypothesis:\n",
        "            if token == eos_token_id:\n",
        "                # If the sequence ends with an EOS token, add it to completed translations\n",
        "                completed_translations.append((sequence, log_prob))\n",
        "                continue\n",
        "\n",
        "            # Prepare input tensor for the current hypothesis\n",
        "            tgt_tensor = torch.tensor([sequence]).to(torch.int64).to(device)  # Add batch dimension\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = model(src_tensor, tgt_tensor)\n",
        "\n",
        "            # Get the top-k tokens and their log probabilities\n",
        "            topk_tokens = output[0, -1, :].topk(beam_width)\n",
        "            for next_token, next_log_prob in zip(topk_tokens.indices, topk_tokens.values):\n",
        "                new_sequence = sequence + [next_token.item()]\n",
        "                new_log_prob = log_prob + next_log_prob.item()\n",
        "\n",
        "                new_hypotheses.append((next_token.item(), new_sequence, new_log_prob))\n",
        "\n",
        "        # Sort hypotheses by log probability and keep the top-k\n",
        "        new_hypotheses.sort(key=lambda x: x[2], reverse=True)\n",
        "        initial_hypothesis = new_hypotheses[:beam_width]\n",
        "\n",
        "    # Add any remaining completed translations\n",
        "    completed_translations.extend([(sequence, log_prob) for _, sequence, log_prob in initial_hypothesis if sequence[-1] == eos_token_id])\n",
        "\n",
        "    if not completed_translations:\n",
        "        return \"Translation not found\"  # No translations found\n",
        "\n",
        "    # Sort completed translations by log probability and return the best one\n",
        "    completed_translations.sort(key=lambda x: x[1], reverse=True)\n",
        "    best_translation = completed_translations[0][0]\n",
        "\n",
        "    # Decode the best translation back to words\n",
        "    translated_sentence = tgt_tokenizer.decode(best_translation)  # Skip start and end tokens\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "# Now let's translate!\n",
        "translated = beam_search(transformer, \"What's your favorite color?\", english_tokenizer, spanish_tokenizer)\n",
        "print(translated)\n",
        "\n",
        "translated = beam_search(transformer, \"Reading a book is a great way to relax.\", english_tokenizer, spanish_tokenizer)\n",
        "print(translated)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}